import json\nimport inspect\nimport traceback\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nfrom typing import Callable, Union, Optional, Any, get_origin, get_args\nfrom pprint import pprint\nfrom pydantic import BaseModel, ValidationError\nfrom dataclasses import dataclass, field\nfrom llm_easy_tools.schema_generator import get_name, parameters_basemodel_from_function, LLMFunction\nfrom llm_easy_tools.types import ChatCompletion, ChatCompletionMessageToolCall, ChatCompletionMessage, Function\n\nclass NoMatchingTool(Exception):\n    def __init__(self, message):\n        self.message = message\n        super().__init__(self.message)\n\n@dataclass\nclass ToolResult:\n    """\n    Represents the result of a tool invocation within the ToolBox framework.\n\n    Attributes:\n        tool_call_id (str): A unique identifier for the tool call.\n        name (str): The name of the tool that was called.\n        output (Optional[Union[str, BaseModel]]): The output generated by the tool call, if any.\n        error (Optional[Exception]): An error message if the tool call failed.\n        stack_trace (Optional[str]): The stack trace if the tool call failed.\n        soft_errors (list[Exception]): A list of non-critical error messages encountered during the tool call.\n        prefix (Optional[BaseModel]): The Pydantic model instance used as a prefix in the tool call, if applicable.\n        tool (Optional[Union[Callable, BaseModel]]): The function or model that was called.\n\n    Methods:\n        to_message(): Converts the ToolResult into a dictionary suitable for returning to a chat interface.\n    """\n    tool_call_id: str\n    name: str\n    output: Optional[Any] = None\n    arguments: Optional[dict[str, Any]] = None\n    error: Optional[Exception] = None\n    stack_trace: Optional[str] = None\n    soft_errors: list[Exception] = field(default_factory=list)\n    prefix: Optional[BaseModel] = None\n    tool: Optional[Union[Callable, BaseModel]] = None\n\n    def to_message(self) -> dict[str, str]:\n        if self.error is not None:\n            content = str(self.error)\n        elif self.output is None:\n            content = ''\n        elif isinstance(self.output, BaseModel):\n            content = f\"{self.name} created\"\n        else:\n            content = str(self.output)\n        return {\n            \"role\": \"tool\",\n            \"tool_call_id\": self.tool_call_id,\n            \"name\": self.name,\n            \"content\": content,\n        }\n\ndef process_tool_call(tool_call, functions_or_models, fix_json_args=True, case_insensitive=False) -> ToolResult:\n    function_call = tool_call.function\n    tool_name = function_call.name\n    args = function_call.arguments\n    soft_errors: list[Exception] = []\n    error = None\n    stack_trace = None\n    output = None\n    try:\n        tool_args = json.loads(args)\n    except json.decoder.JSONDecodeError as e:\n        if fix_json_args:\n            soft_errors.append(e)\n            args = args.replace(", }", "}").replace(",}", "}")\n            tool_args = json.loads(args)\n        else:\n            stack_trace = traceback.format_exc()\n            return ToolResult(tool_call_id=tool_call.id, name=tool_name, error=e, stack_trace=stack_trace)\n\n    tool = None\n    for f in functions_or_models:\n        if get_name(f, case_insensitive=case_insensitive) == tool_name:\n            tool = f\n            try:\n                output, new_soft_errors = _process_unpacked(f, tool_args, fix_json_args=fix_json_args)\n                soft_errors.extend(new_soft_errors)\n            except Exception as e:\n                error = e\n                stack_trace = traceback.format_exc()\n            break\n    else:\n        error = NoMatchingTool(f\"Function {tool_name} not found\")\n    result = ToolResult(\n        tool_call_id=tool_call.id, \n        name=tool_name,\n        arguments=tool_args,\n        output=output, \n        error=error,\n        stack_trace=stack_trace,\n        soft_errors=soft_errors,\n        prefix=None,\n        tool=tool,\n    )\n    return result\n\ndef split_string_to_list(s: str) -> list[str]:\n    try:\n        return json.loads(s)\n    except json.JSONDecodeError:\n        return [item.strip() for item in s.split(",")]\n\ndef _process_unpacked(function, tool_args={}, fix_json_args=True):\n    if isinstance(function, LLMFunction):\n        function = function.func\n    model = parameters_basemodel_from_function(function)\n    soft_errors = []\n    if fix_json_args:\n        for field, field_info in model.model_fields.items():\n            field_annotation = field_info.annotation\n            if _is_list_type(field_annotation):\n                if field in tool_args and isinstance(tool_args[field], str):\n                    tool_args[field] = split_string_to_list(tool_args[field])\n                    soft_errors.append(f\"Fixed JSON decode error for field {field}\")\n    model_instance = model(**tool_args)\n    args = {}\n    for field, _ in model.model_fields.items():\n        args[field] = getattr(model_instance, field)\n    return function(**args), soft_errors\n\ndef _is_list_type(annotation):\n    origin = get_origin(annotation)\n    args = get_args(annotation)\n    if origin is list:\n        return True\n    elif origin in (Union, Optional):\n        return any(_is_list_type(arg) for arg in args)\n    return False\n\ndef process_response(response: ChatCompletion, functions: list[Union[Callable, LLMFunction]], choice_num=0, **kwargs) -> list[ToolResult]:\n    \"\"\"\n    Processes a ChatCompletion response, executing contained tool calls.\n    For each tool call matches a function from the 'functions' list by name.\n    The result of the tool call is returned as a ToolResult object.\n    If the tool call raises an exception, that exception is saved in the 'error' field in the result.\n\n    Args:\n        response (ChatCompletion): The response object containing tool calls.\n        functions (list[Callable]): A list of functions or pydantic models to call.\n        choice_num (int, optional): The index of the choice to process from the response. Defaults to 0.\n\n    Returns:\n        list[ToolResult]: A list of ToolResult objects, each representing the outcome of a processed tool call.\n    \"\"\"\n    message = response.choices[choice_num].message\n    return process_message(message, functions, **kwargs)\n\ndef process_message(\n    message: ChatCompletionMessage,\n    functions: list[Union[Callable, LLMFunction]],\n    fix_json_args=True,\n    case_insensitive=False,\n    executor: Union[ThreadPoolExecutor, ProcessPoolExecutor, None]=None\n) -> list[ToolResult]:\n    results = []\n    if hasattr(message, 'function_call') and (function_call := message.function_call):\n        tool_calls = [ChatCompletionMessageToolCall(id='A', function=Function(name=function_call.name, arguments=function_call.arguments), type='function')]\n    elif hasattr(message, 'tool_calls') and message.tool_calls:\n        tool_calls = message.tool_calls\n    else:\n        tool_calls = []\n    if not tool_calls:\n        return []\n    args_list = [(tool_call, functions, fix_json_args, case_insensitive) for tool_call in tool_calls]\n    if executor:\n        results = list(executor.map(lambda args: process_tool_call(*args), args_list))\n    else:\n        results = list(map(lambda args: process_tool_call(*args), args_list)) \n    return results\n\ndef process_one_tool_call(\n        response: ChatCompletion,\n        functions: list[Union[Callable, LLMFunction]],\n        index: int = 0,\n        fix_json_args=True,\n        case_insensitive=False\n    ) -> Optional[ToolResult]:\n    \"\"\"\n    Processes a single tool call from a ChatCompletion response at the specified index.\n    \"\"\"\n    tool_calls = _get_tool_calls(response)\n    if not tool_calls or index >= len(tool_calls):\n        return None\n    return process_tool_call(tool_calls[index], functions, fix_json_args, case_insensitive)\n\ndef _get_tool_calls(response: ChatCompletion) -> list[ChatCompletionMessageToolCall]:\n    if hasattr(response.choices[0].message, 'function_call') and (function_call := response.choices[0].message.function_call):\n        return [ChatCompletionMessageToolCall(id='A', function=Function(name=function_call.name, arguments=function_call.arguments), type='function')]\n    elif hasattr(response.choices[0].message, 'tool_calls') and response.choices[0].message.tool_calls:\n        return response.choices[0].message.tool_calls\n    return []\n\nif __name__ == "__main__":\n    from llm_easy_tools.types import mk_chat_with_tool_call\n\n    def original_function():\n        return 'Result of function_decorated'\n\n    function_decorated = LLMFunction(original_function, name="altered_name")\n\n    class ExampleClass:\n        def simple_method(self, count: int, size: float):\n            \"\"\"simple method does something\"\"\"\n            return 'Result of simple_method'\n\n    example_object = ExampleClass()\n\n    class User(BaseModel):\n        name: str\n        email: str\n\n    pprint(process_response(mk_chat_with_tool_call('altered_name', {}), [function_decorated]))\n    call_to_altered_name = mk_chat_with_tool_call('altered_name', {}).choices[0].message.tool_calls[0]\n    pprint(call_to_altered_name)\n    pprint(process_tool_call(call_to_altered_name, [function_decorated]))\n\n    call_to_simple_method = mk_chat_with_tool_call('simple_method', {"count": 1, "size": 2.2}).choices[0].message.tool_calls[0]\n    pprint(process_tool_call(call_to_simple_method, [example_object.simple_method]))\n\n    call_to_model = mk_chat_with_tool_call('User', {"name": 'John', "email": 'john@example.com'}).choices[0].message.tool_calls[0]\n    pprint(process_tool_call(call_to_model, [User]))\n